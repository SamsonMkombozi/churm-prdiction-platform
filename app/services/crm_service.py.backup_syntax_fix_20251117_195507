"""
DISCONNECTION-BASED Enhanced CRM Service with Complete Data Storage
app/services/crm_service.py

ðŸ”¥ ENHANCED: Disconnection-date-based churn prediction (90/60 day business rules)
ðŸ“Š STORAGE: Saves payments, tickets, usage stats to SQLite from PostgreSQL
âš¡ OPTIMIZED: Fast separate queries for 67k+ customers

Business Logic:
- HIGH: 90+ days after disconnection with no new payments
- MEDIUM: 60+ days after disconnection with no/inconsistent payments
- LOW: Recent disconnection or good payment behavior

Author: Samson David - Mawingu Group  
Date: November 2024
"""

import psycopg2
import psycopg2.extras
from datetime import datetime, timedelta
from flask import current_app
from sqlalchemy.exc import IntegrityError
from app.extensions import db
from app.models.customer import Customer
from app.models.payment import Payment
from app.models.ticket import Ticket
from app.models.prediction import Prediction
from app.models.company import Company
from app.services.prediction_service import EnhancedChurnPredictionService
import traceback
import time
import logging
import json

logger = logging.getLogger(__name__)

class DisconnectionBasedCRMService:
    """Enhanced CRM Service with disconnection-based churn prediction and complete data storage"""
    
    def __init__(self, company):
        self.company = company
        self.connection = None
        
        # Initialize prediction service
        self.prediction_service = EnhancedChurnPredictionService()
        
        logger.info(f"Initializing DISCONNECTION-BASED CRM Service for: {company.name}")
        
        # Customer lookup cache
        self.customer_cache = {}
        self.customer_name_cache = {}
        
        # Enhanced tracking with disconnection analytics
        self.sync_stats = {
            'start_time': None,
            'customers': {'new': 0, 'updated': 0, 'cached': 0, 'errors': 0, 'disconnected': 0},
            'payments': {'new': 0, 'updated': 0, 'stored': 0, 'errors': 0},
            'tickets': {'new': 0, 'updated': 0, 'stored': 0, 'errors': 0},
            'usage_stats': {'new': 0, 'updated': 0, 'stored': 0, 'errors': 0},
            'predictions': {'generated': 0, 'high_risk': 0, 'medium_risk': 0, 'low_risk': 0, 'errors': 0},
            'disconnection_analysis': {
                'total_disconnected': 0,
                'high_risk_disconnected': 0,
                'medium_risk_disconnected': 0,
                'days_90_plus': 0,
                'days_60_plus': 0
            },
            'cache_performance': {'hits': 0, 'misses': 0, 'build_time': 0},
            'total_records': 0,
            'sync_duration': 0,
            'connection_method': 'postgresql_disconnection_based'
        }
        
        # Enhanced customer data for predictions
        self.enhanced_customers = {}
        
        # Performance tracking
        self.query_times = {}
        
        # Storage tracking
        self.stored_payments = []
        self.stored_tickets = []
        self.stored_usage = []
    
    def get_connection_info(self):
        """Get connection info with disconnection-based prediction capabilities"""
        
        logger.info(f"=== DISCONNECTION-BASED CRM CONNECTION INFO ===")
        
        postgresql_configured = self.company.has_postgresql_config()
        api_configured = self.company.has_api_config()
        
        return {
            'postgresql_configured': postgresql_configured,
            'api_configured': api_configured,
            'preferred_method': 'postgresql' if postgresql_configured else 'api',
            'prediction_enabled': True,
            'disconnection_based_predictions': True,
            'payment_storage': True,
            'ticket_storage': True,
            'usage_storage': True,
            'business_logic': '90/60 day disconnection rules',
            'tables_mapped': ['crm_customers', 'crm_tickets', 'nav_mpesa_transactions', 'spl_statistics']
        }
    
    def sync_data_selective(self, sync_options=None):
        """Enhanced selective sync with disconnection-based predictions and data storage"""
        
        if sync_options is None:
            sync_options = {
                'sync_customers': True,
                'sync_payments': True,
                'sync_tickets': True,
                'sync_usage': True,
                'generate_predictions': True,
                'store_payment_records': True,  # ðŸ”§ NEW: Store individual payments
                'store_ticket_records': True,   # ðŸ”§ NEW: Store individual tickets
                'store_usage_records': True     # ðŸ”§ NEW: Store usage statistics
            }
        
        if 'generate_predictions' not in sync_options:
            sync_options['generate_predictions'] = True
        
        self.sync_stats['start_time'] = time.time()
        
        logger.info(f"=== DISCONNECTION-BASED SYNC WITH DATA STORAGE STARTED ===")
        logger.info(f"Sync options: {sync_options}")
        
        try:
            self._safe_session_rollback()
            self.company.mark_sync_started()
            
            connection_info = self.get_connection_info()
            
            if connection_info['preferred_method'] == 'postgresql':
                return self._disconnection_based_postgresql_sync(sync_options)
            else:
                return {
                    'success': False,
                    'message': 'PostgreSQL required for disconnection-based sync',
                    'recommendation': 'Configure PostgreSQL connection in company settings'
                }
                
        except Exception as e:
            error_msg = f"Disconnection-based sync failed: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            
            self._safe_session_rollback()
            self.company.mark_sync_failed(error_msg)
            
            return {
                'success': False,
                'message': error_msg,
                'stats': self.sync_stats,
                'query_performance': self.query_times
            }
    
    def _disconnection_based_postgresql_sync(self, sync_options):
        """Main sync method with disconnection-based churn prediction"""
        
        logger.info("=== DISCONNECTION-BASED POSTGRESQL SYNC ===")
        
        try:
            # Get connection
            conn = self._get_postgresql_connection()
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            logger.info("PostgreSQL connection established for disconnection-based sync")
            
            # Build customer cache
            self._build_comprehensive_customer_cache()
            
            # STEP 1: Enhanced customer sync with disconnection analysis
            if sync_options.get('sync_customers', True):
                logger.info("[1/6] Disconnection-based customer sync with enhanced analytics...")
                self._disconnection_based_customer_sync(cursor)
            
            # STEP 2: Store payment records to SQLite
            if sync_options.get('sync_payments', True) and sync_options.get('store_payment_records', True):
                logger.info("[2/6] Storing payment records to SQLite...")
                logger.info("   â†’ Storing individual payment records...")
            
            try:
                # Get payment data from PostgreSQL
                cursor.execute(payment_query)
                payment_results = cursor.fetchall()
                
                # Convert to list of dictionaries
                payment_data = []
                for row in payment_results:
                    payment_data.append({
                        'customer_id': row[0],
                        'total_payments': row[1],
                        'successful_payments': row[2],
                        'total_paid_amount': row[3],
                        'tx_time': row[4],
                        'payment_consistency_score': row[5],
                        'posted_to_ledgers': 1,
                        'payment_method': 'mpesa'
                    })
                
                # Store in SQLite
                self._store_payment_records(payment_data)
                
            except Exception as e:
                logger.error(f"Payment storage failed: {e}")

            logger.info("[3/6] Storing ticket records to SQLite...")
            logger.info("   â†’ Storing individual ticket records...")
            
            try:
                # Get ticket data from PostgreSQL
                cursor.execute(ticket_query)
                ticket_results = cursor.fetchall()
                
                # Convert to list of dictionaries
                ticket_data = []
                for row in ticket_results:
                    ticket_data.append({
                        'customer_id': row[0],
                        'total_tickets': row[1],
                        'status': 'open',
                        'priority': 'medium',
                        'subject': 'Support Ticket',
                        'created_at': datetime.utcnow()
                    })
                
                # Store in SQLite
                self._store_ticket_records(ticket_data)
                
            except Exception as e:
except Exception as e:
    logger.error(f"Ticket storage failed: {e}")

logger.info("[4/6] Storing usage statistics to SQLite...")
self._store_usage_statistics(cursor)
            # STEP 5: Generate disconnection-based churn predictions
            if sync_options.get('generate_predictions', True):
                logger.info("[5/6] Generating disconnection-based churn predictions...")
                self._generate_disconnection_based_predictions()
            
            # STEP 6: Final analytics and summary
            logger.info("[6/6] Generating disconnection analytics summary...")
            self._analyze_disconnection_patterns()
            
            # Calculate performance stats
            elapsed_time = time.time() - self.sync_stats['start_time']
            total_records = (
                self.sync_stats['customers']['new'] + self.sync_stats['customers']['updated'] +
                self.sync_stats['payments']['stored'] + self.sync_stats['tickets']['stored'] +
                self.sync_stats['usage_stats']['stored'] + self.sync_stats['predictions']['generated']
            )
            
            self.sync_stats.update({
                'sync_duration': round(elapsed_time, 2),
                'total_records': total_records,
                'records_per_second': round(total_records / elapsed_time, 2) if elapsed_time > 0 else 0
            })
            
            # Final commit
            self._safe_session_commit()
            self.company.mark_sync_completed()
            
            # Success report
            logger.info(f"=== DISCONNECTION-BASED SYNC COMPLETED ===")
            logger.info(f"Total records: {total_records:,}")
            logger.info(f"Disconnected customers: {self.sync_stats['disconnection_analysis']['total_disconnected']:,}")
            logger.info(f"High risk (90+ days): {self.sync_stats['disconnection_analysis']['high_risk_disconnected']:,}")
            logger.info(f"Predictions generated: {self.sync_stats['predictions']['generated']:,}")
            logger.info(f"Duration: {elapsed_time:.1f}s")
            
            return {
                'success': True,
                'message': f'Disconnection-based sync completed! Processed {total_records:,} records with disconnection analysis',
                'stats': self.sync_stats,
                'performance': {
                    'sync_duration': self.sync_stats['sync_duration'],
                    'records_per_second': self.sync_stats['records_per_second'],
                    'optimization_used': 'Disconnection-based churn prediction with data storage'
                },
                'query_performance': self.query_times,
                'disconnection_summary': self.sync_stats['disconnection_analysis'],
                'prediction_summary': {
                    'total_predictions': self.sync_stats['predictions']['generated'],
                    'high_risk': self.sync_stats['predictions']['high_risk'],
                    'medium_risk': self.sync_stats['predictions']['medium_risk'],
                    'low_risk': self.sync_stats['predictions']['low_risk'],
                    'disconnection_based': True
                }
            }
            
        except Exception as e:
            logger.error(f"Disconnection-based sync failed: {str(e)}")
            logger.error(traceback.format_exc())
            
            self._safe_session_rollback()
            self.company.mark_sync_failed(str(e))
            
            raise Exception(f"Disconnection-based sync failed: {str(e)}")
        
        finally:
            if self.connection:
                self.connection.close()
                self.connection = None
    
    def _disconnection_based_customer_sync(self, cursor):
        """Enhanced customer sync with disconnection date analysis"""
        
        try:
            # ðŸš€ STEP 1: Get basic customer data with disconnection info
            start_time = time.time()
            
            logger.info("   â†’ Getting customer data with disconnection analysis...")
            customer_query = """
                SELECT 
                    c.id,
                    c.customer_name,
                    c.customer_phone,
                    c.customer_balance,
                    c.status,
                    c.connection_status,
                    c.date_installed,
                    c.created_at,
                    c.churned_date,  -- ðŸ”§ CRITICAL: Text-based disconnection date field
                    c.splynx_location,
                    -- Calculate days since disconnection with text conversion
                    CASE 
                        WHEN c.churned_date IS NOT NULL 
                        AND c.churned_date != '' 
                        AND c.churned_date != '0001-01-01'
                        AND c.churned_date::date < CURRENT_DATE THEN 
                            (CURRENT_DATE - c.churned_date::date)::INTEGER
                        ELSE NULL 
                    END as days_since_disconnection
                FROM crm_customers c
                WHERE c.customer_name IS NOT NULL 
                AND c.customer_name != ''
                AND c.customer_name NOT ILIKE 'test%'
                AND c.customer_name != 'None'
                ORDER BY 
                    CASE 
                        WHEN c.churned_date IS NOT NULL 
                        AND c.churned_date != '' 
                        AND c.churned_date != '0001-01-01' THEN c.churned_date::date 
                        ELSE NULL 
                    END DESC NULLS LAST, 
                    c.id
            """
            
            cursor.execute(customer_query)
            customers_data = cursor.fetchall()
            self.query_times['customers_with_disconnection'] = round(time.time() - start_time, 2)
            
            logger.info(f"   âœ… Retrieved {len(customers_data):,} customers in {self.query_times['customers_with_disconnection']}s")
            
            # Count disconnected customers for analytics
            disconnected_count = sum(1 for c in customers_data if c['churned_date'])
            self.sync_stats['disconnection_analysis']['total_disconnected'] = disconnected_count
            logger.info(f"   ðŸ“Š Found {disconnected_count:,} disconnected customers")
            
            # ðŸš€ STEP 2: Get payment aggregations FAST
            start_time = time.time()
            
            logger.info("   â†’ Getting payment aggregations...")
            payment_query = """
                SELECT 
                    mp.account_no as customer_id,
                    COUNT(*) as total_payments,
                    COUNT(CASE WHEN mp.posted_to_ledgers = 1 AND mp.is_refund = 0 THEN 1 END) as successful_payments,
                    SUM(CASE WHEN mp.posted_to_ledgers = 1 AND mp.is_refund = 0 THEN mp.tx_amount ELSE 0 END) as total_paid_amount,
                    MAX(CASE WHEN mp.posted_to_ledgers = 1 AND mp.is_refund = 0 THEN mp.tx_time END) as last_payment_date,
                    CASE 
                        WHEN COUNT(*) > 0 THEN 
                            COUNT(CASE WHEN mp.posted_to_ledgers = 1 AND mp.is_refund = 0 THEN 1 END)::FLOAT / COUNT(*)::FLOAT
                        ELSE 1.0 
                    END as payment_consistency_score
                FROM nav_mpesa_transactions mp
                WHERE mp.tx_time >= CURRENT_DATE - INTERVAL '2 years'
                GROUP BY mp.account_no
            """
            
            cursor.execute(payment_query)
            payment_data = cursor.fetchall()
            self.query_times['payments'] = round(time.time() - start_time, 2)
            
            # Convert to dict for fast lookup
            payment_dict = {}
            for row in payment_data:
                payment_dict[row['customer_id']] = dict(row)
            
            logger.info(f"   âœ… Retrieved payment data for {len(payment_dict):,} customers in {self.query_times['payments']}s")
            
            # ðŸš€ STEP 3: Get ticket aggregations
            start_time = time.time()
            
            logger.info("   â†’ Getting ticket aggregations...")
            ticket_query = """
                SELECT 
                    t.customer_no as customer_id,
                    COUNT(*) as total_tickets,
                    COUNT(CASE WHEN t.status = 'open' THEN 1 END) as open_tickets,
                    COUNT(CASE WHEN t.priority IN ('high', 'urgent')  THEN 1 END) as complaint_tickets
                FROM crm_tickets t
                WHERE t.created_at >= CURRENT_DATE - INTERVAL '2 years'
                GROUP BY t.customer_no
            """
            
            cursor.execute(ticket_query)
            ticket_data = cursor.fetchall()
            self.query_times['tickets'] = round(time.time() - start_time, 2)
            
            # Convert to dict
            ticket_dict = {}
            for row in ticket_data:
                ticket_dict[row['customer_id']] = dict(row)
            
            logger.info(f"   âœ… Retrieved ticket data for {len(ticket_dict):,} customers in {self.query_times['tickets']}s")
            
            # ðŸš€ STEP 4: Get usage aggregations (for numeric IDs only)
            start_time = time.time()
            
            logger.info("   â†’ Getting usage aggregations...")
            usage_query = """
                SELECT 
                    s.customer_id,
                    COUNT(*) as usage_records,
                    AVG((COALESCE(s.in_bytes, 0) + COALESCE(s.out_bytes, 0)) / 1048576.0) as avg_mb_usage,
                    SUM(COALESCE(s.in_bytes, 0) + COALESCE(s.out_bytes, 0)) as total_bytes
                FROM spl_statistics s
                WHERE s.start_date >= CURRENT_DATE - INTERVAL '2 years'
                GROUP BY s.customer_id
            """
            
            cursor.execute(usage_query)
            usage_data = cursor.fetchall()
            self.query_times['usage'] = round(time.time() - start_time, 2)
            
            # Convert to dict
            usage_dict = {}
            for row in usage_data:
                usage_dict[str(row['customer_id'])] = dict(row)
            
            logger.info(f"   âœ… Retrieved usage data for {len(usage_dict):,} customers in {self.query_times['usage']}s")
            
            # ðŸš€ STEP 5: Process customers with disconnection-based analysis
            start_time = time.time()
            
            logger.info("   â†’ Processing customers with disconnection-based churn analysis...")
            
            batch_size = 100
            for i in range(0, len(customers_data), batch_size):
                batch = customers_data[i:i + batch_size]
                
                for customer_row in batch:
                    try:
                        customer_data = dict(customer_row)
                        crm_id = str(customer_data['id'])
                        
                        # Get aggregated data
                        payment_info = payment_dict.get(crm_id, {})
                        ticket_info = ticket_dict.get(crm_id, {})
                        usage_info = usage_dict.get(crm_id, {})
                        
                        # Combine all data
                        combined_data = {**customer_data, **payment_info, **ticket_info, **usage_info}
                        
                        # Calculate enhanced metrics with disconnection analysis
                        enhanced_data = self._calculate_disconnection_based_metrics(combined_data)
                        
                        # Check if customer exists
                        customer = Customer.query.filter_by(
                            company_id=self.company.id,
                            crm_customer_id=crm_id
                        ).first()
                        
                        if customer:
                            self._update_customer_with_disconnection_data(customer, enhanced_data)
                            self.sync_stats['customers']['updated'] += 1
                        else:
                            customer = self._create_customer_with_disconnection_data(enhanced_data)
                            self.sync_stats['customers']['new'] += 1
                        
                        # Track disconnected customers
                        if enhanced_data.get('disconnection_date'):
                            self.sync_stats['customers']['disconnected'] += 1
                            
                            # Track risk by disconnection period
                            days_disconnected = enhanced_data.get('days_since_disconnection', 0)
                            if days_disconnected >= 90:
                                self.sync_stats['disconnection_analysis']['days_90_plus'] += 1
                            elif days_disconnected >= 60:
                                self.sync_stats['disconnection_analysis']['days_60_plus'] += 1
                        
                        # Store for predictions
                        self.enhanced_customers[crm_id] = enhanced_data
                        
                        # Update cache
                        self.customer_cache[crm_id] = customer.id
                        self.customer_name_cache[crm_id] = enhanced_data['customer_name']
                        self.sync_stats['customers']['cached'] += 1
                        
                    except Exception as e:
                        logger.warning(f"Customer {customer_row.get('id')} error: {e}")
                        self.sync_stats['customers']['errors'] += 1
                        continue
                
                # Commit batch
                try:
                    db.session.commit()
                except Exception as e:
                    logger.warning(f"Batch commit failed: {e}")
                    db.session.rollback()
                
                # Progress logging
                if (i // batch_size + 1) % 50 == 0:
                    logger.info(f"   Processed {i + len(batch):,} customers...")
            
            self.query_times['processing'] = round(time.time() - start_time, 2)
            
            total_time = sum(self.query_times.values())
            logger.info(f"âœ… Disconnection-based customer sync completed in {total_time:.1f}s")
            logger.info(f"   Stats: {self.sync_stats['customers']}")
            logger.info(f"   Disconnected: {self.sync_stats['customers']['disconnected']:,}")
            
        except Exception as e:
            logger.error(f"âŒ Disconnection-based customer sync failed: {e}")
            raise
    
    def _calculate_disconnection_based_metrics(self, combined_data):
        """Calculate customer metrics with disconnection-based churn analysis"""
        
        try:
            current_date = datetime.utcnow()
            
            # Basic customer info
            crm_id = str(combined_data['id'])
            customer_name = combined_data.get('customer_name', 'Unknown Customer')
            phone = combined_data.get('customer_phone', '')
            balance = float(combined_data.get('customer_balance', 0) or 0)
            
            # ðŸ“… CRITICAL: Disconnection analysis with text date handling
            churned_date = combined_data.get('churned_date')
            disconnection_date = None
            days_since_disconnection = 0
            
            if churned_date and churned_date != '' and churned_date != '0001-01-01':
                try:
                    # Handle text-based dates from PostgreSQL
                    if isinstance(churned_date, str):
                        # Try different date formats
                        date_formats = [
                            '%Y-%m-%d %H:%M:%S',  # 2024-06-18 13:20:23
                            '%Y-%m-%d',           # 2024-06-18
                            '%d/%m/%Y',           # 18/06/2024
                            '%m/%d/%Y'            # 06/18/2024
                        ]
                        
                        for date_format in date_formats:
                            try:
                                disconnection_date = datetime.strptime(churned_date, date_format)
                                break
                            except ValueError:
                                continue
                        
                        # If no format worked, try basic parsing
                        if disconnection_date is None and len(churned_date) >= 10:
                            try:
                                disconnection_date = datetime.strptime(churned_date[:10], '%Y-%m-%d')
                            except:
                                pass
                    else:
                        disconnection_date = churned_date
                    
                    if disconnection_date and disconnection_date.year > 2000:  # Sanity check
                        days_since_disconnection = (current_date - disconnection_date).days
                    else:
                        disconnection_date = None
                        
                except Exception as e:
                    logger.warning(f"Could not parse disconnection date '{churned_date}': {e}")
                    disconnection_date = None
            
            # Payment metrics
            total_payments = int(combined_data.get('total_payments', 0) or 0)
            successful_payments = int(combined_data.get('successful_payments', 0) or 0)
            failed_payments = total_payments - successful_payments
            total_paid_amount = float(combined_data.get('total_paid_amount', 0) or 0)
            last_payment_date = combined_data.get('last_payment_date')
            payment_consistency_score = float(combined_data.get('payment_consistency_score', 1.0) or 1.0)
            
            # Calculate days since last payment
            days_since_last_payment = 999
            if last_payment_date:
                days_since_last_payment = (current_date - last_payment_date).days
            
            # Calculate tenure
            date_installed = combined_data.get('date_installed')
            tenure_months, signup_date = self._safe_date_calculation(date_installed, current_date)
            
            # Support metrics
            total_tickets = int(combined_data.get('total_tickets', 0) or 0)
            open_tickets = int(combined_data.get('open_tickets', 0) or 0)
            complaint_tickets = int(combined_data.get('complaint_tickets', 0) or 0)
            
            # Usage metrics
            usage_records = int(combined_data.get('usage_records', 0) or 0)
            avg_mb_usage = float(combined_data.get('avg_mb_usage', 0) or 0)
            total_bytes = int(combined_data.get('total_bytes', 0) or 0)
            
            # ðŸ”¥ APPLY DISCONNECTION-BASED CHURN LOGIC
            churn_assessment = self._assess_disconnection_based_churn_risk(
                disconnection_date, days_since_disconnection, last_payment_date, 
                total_payments, payment_consistency_score, current_date
            )
            
            # Create enhanced customer data
            enhanced_data = {
                # Basic info
                'customer_id': crm_id,
                'id': crm_id,
                'crm_customer_id': crm_id,
                'customer_name': customer_name,
                'phone': phone,
                'email': '',
                'address': combined_data.get('splynx_location', ''),
                'signup_date': signup_date,
                'tenure_months': tenure_months,
                'outstanding_balance': abs(balance),
                'status': combined_data.get('status', 'active'),
                'connection_status': combined_data.get('connection_status', ''),
                
                # ðŸ“… DISCONNECTION DATA
                'disconnection_date': disconnection_date,
                'days_since_disconnection': days_since_disconnection,
                'churned_date': disconnection_date.strftime('%Y-%m-%d') if disconnection_date else None,
                
                # Payment data
                'total_payments': total_payments,
                'successful_payments': successful_payments,
                'failed_payments': failed_payments,
                'total_paid_amount': total_paid_amount,
                'last_payment_date': last_payment_date.strftime('%Y-%m-%d') if last_payment_date else None,
                'days_since_last_payment': days_since_last_payment,
                'payment_consistency_score': payment_consistency_score,
                
                # Support data
                'total_tickets': total_tickets,
                'open_tickets': open_tickets,
                'complaint_tickets': complaint_tickets,
                
                # Usage data
                'usage_records': usage_records,
                'avg_data_usage': avg_mb_usage,
                'total_data_consumed': total_bytes,
                
                # ðŸ”¥ DISCONNECTION-BASED CHURN PREDICTION
                'churn_risk_assessment': churn_assessment,
                'predicted_churn_risk': churn_assessment['risk_level'],
                'churn_probability': churn_assessment['probability'],
                'risk_reasoning': churn_assessment['reasoning'],
                'disconnection_risk_level': churn_assessment['disconnection_status'],
                
                # Service info
                'service_plan': 'Standard',
                'monthly_charges': 50000.0,
                'total_charges': max(abs(balance) + total_paid_amount, 600000),
                
                # ML compatibility
                'months_stayed': tenure_months,
                'number_of_payments': successful_payments,
                'missed_payments': failed_payments,
                'customer_number': crm_id
            }
            
            return enhanced_data
            
        except Exception as e:
            logger.error(f"Error calculating disconnection metrics for {combined_data.get('id')}: {e}")
            return {
                'customer_id': str(combined_data.get('id', 'unknown')),
                'customer_name': combined_data.get('customer_name', 'Unknown'),
                'churn_probability': 0.5,
                'predicted_churn_risk': 'medium',
                'disconnection_date': None,
                'days_since_disconnection': 0
            }
    
    def _assess_disconnection_based_churn_risk(self, disconnection_date, days_disconnected, 
                                             last_payment_date, total_payments, success_rate, current_date):
        """
        Assess churn risk based on disconnection date and payment behavior
        
        Business Rules:
        - HIGH: 90+ days after disconnection with no new payments
        - MEDIUM: 60+ days after disconnection with no/inconsistent payments
        - LOW: Recent disconnection or good payment behavior
        """
        
        risk_assessment = {
            'risk_level': 'low',
            'probability': 0.1,
            'reasoning': [],
            'disconnection_status': 'active',
            'days_since_disconnection': days_disconnected
        }
        
        if disconnection_date:
            # Customer is disconnected
            risk_assessment['disconnection_status'] = 'disconnected'
            
            # Count payments after disconnection (estimate)
            payments_after_disconnection = 0
            if last_payment_date and last_payment_date > disconnection_date:
                payments_after_disconnection = max(1, int(total_payments * 0.3))  # Estimate
            
            # HIGH RISK: 90+ days disconnected with no new payments
            if days_disconnected >= 90:
                if payments_after_disconnection == 0:
                    risk_assessment['risk_level'] = 'high'
                    risk_assessment['probability'] = min(0.8 + (days_disconnected - 90) / 365, 0.95)
                    risk_assessment['reasoning'].append(f"Disconnected {days_disconnected} days with no payments (>90 days)")
                else:
                    risk_assessment['risk_level'] = 'medium'
                    risk_assessment['probability'] = 0.6
                    risk_assessment['reasoning'].append(f"Disconnected {days_disconnected} days despite payments")
            
            # MEDIUM RISK: 60+ days disconnected OR inconsistent payments
            elif days_disconnected >= 60:
                if payments_after_disconnection == 0 or success_rate < 0.7:
                    risk_assessment['risk_level'] = 'medium'
                    risk_assessment['probability'] = 0.5 + (days_disconnected - 60) / 300
                    risk_assessment['reasoning'].append(f"Disconnected {days_disconnected} days with poor payment behavior")
                else:
                    risk_assessment['risk_level'] = 'low'
                    risk_assessment['probability'] = 0.25
                    risk_assessment['reasoning'].append(f"Disconnected but maintaining payments")
            
            # RECENT DISCONNECTION: < 60 days
            else:
                if payments_after_disconnection > 0:
                    risk_assessment['risk_level'] = 'low'
                    risk_assessment['probability'] = 0.15
                    risk_assessment['reasoning'].append(f"Recently disconnected ({days_disconnected} days) with payments")
                else:
                    risk_assessment['risk_level'] = 'medium'
                    risk_assessment['probability'] = 0.35
                    risk_assessment['reasoning'].append(f"Recently disconnected ({days_disconnected} days) no payments")
        
        else:
            # Customer is not disconnected - use payment-based logic
            days_since_payment = 999
            if last_payment_date:
                days_since_payment = (current_date - last_payment_date).days
            
            if days_since_payment >= 90:
                risk_assessment['risk_level'] = 'high'
                risk_assessment['probability'] = 0.7
                risk_assessment['reasoning'].append(f"Active but no payments for {days_since_payment} days")
            elif days_since_payment >= 60:
                risk_assessment['risk_level'] = 'medium'
                risk_assessment['probability'] = 0.4
                risk_assessment['reasoning'].append(f"Active but no payments for {days_since_payment} days")
            elif total_payments == 0:
                risk_assessment['risk_level'] = 'high'
                risk_assessment['probability'] = 0.8
                risk_assessment['reasoning'].append("Active customer with no payment history")
            else:
                risk_assessment['risk_level'] = 'low'
                risk_assessment['probability'] = 0.1
                risk_assessment['reasoning'].append("Active customer with good payment behavior")
        
        return risk_assessment
    
    def _store_payment_records(self, cursor):
        """Store individual payment records to SQLite Payment table"""
        
        try:
            logger.info("   â†’ Storing individual payment records...")
            start_time = time.time()
            
            # Get payment records for customers in our system
            customer_ids = "','".join(self.customer_cache.keys())
            
            payment_query = f"""
                SELECT 
                    mp.id,
                    mp.account_no as customer_id,
                    mp.tx_time as payment_date,
                    mp.tx_amount as amount,
                    mp.transaction_id,
                    mp.posted_to_ledgers,
                    mp.is_refund,
                    'completed' as status
                FROM nav_mpesa_transactions mp
                WHERE mp.account_no IN ('{customer_ids}')
                AND mp.tx_time >= CURRENT_DATE - INTERVAL '2 years'
                ORDER BY mp.tx_time DESC
                LIMIT 10000
            """
            
            cursor.execute(payment_query)
            payment_records = cursor.fetchall()
            
            stored_count = 0
            
            # Store payments in batches
            for payment_row in payment_records:
                try:
                    payment_data = dict(payment_row)
                    customer_id = payment_data['customer_id']
                    
                    # Get internal customer ID
                    internal_customer_id = self.customer_cache.get(str(customer_id))
                    if not internal_customer_id:
                        continue
                    
                    # Check if payment already exists
                    existing_payment = Payment.query.filter_by(
                        company_id=self.company.id,
                        transaction_id=payment_data.get('transaction_id', f"mp_{payment_data['id']}")
                    ).first()
                    
                    if not existing_payment:
                        # Create new payment record
                        payment = Payment(
                            company_id=self.company.id,
                            customer_id=internal_customer_id,
                            amount=float(payment_data.get('amount', 0) or 0),
                            payment_date=payment_data['payment_date'],
                            payment_method=payment_data.get('payment_method', 'M-Pesa'),
                            transaction_id=payment_data.get('transaction_id', f"mp_{payment_data['id']}"),
                            status='completed' if payment_data.get('posted_to_ledgers') == 1 else 'pending',
                            created_at=datetime.utcnow()
                        )
                        
                        db.session.add(payment)
                        stored_count += 1
                        
                        if stored_count % 100 == 0:
                            db.session.commit()
                    
                except Exception as e:
                    logger.warning(f"Payment storage error: {e}")
                    self.sync_stats['payments']['errors'] += 1
                    continue
            
            # Final commit
            db.session.commit()
            
            self.sync_stats['payments']['stored'] = stored_count
            self.query_times['payment_storage'] = round(time.time() - start_time, 2)
            
            logger.info(f"   âœ… Stored {stored_count:,} payment records in {self.query_times['payment_storage']}s")
            
        except Exception as e:
            logger.error(f"Payment storage failed: {e}")
            self.sync_stats['payments']['errors'] += 1
    
    def _store_ticket_records(self, cursor):
        """Store individual ticket records to SQLite Ticket table"""
        
        try:
            logger.info("   â†’ Storing individual ticket records...")
            start_time = time.time()
            
            # Get ticket records for customers in our system
            customer_ids = "','".join(self.customer_cache.keys())
            
            ticket_query = f"""
                SELECT 
                    t.id,
                    t.customer_no as customer_id,
                    t.title,
                    t.description,
                    t.status,
                    t.priority,
                    ,
                    t.created_at,
                    t.updated_at,
                    COALESCE(t.ticket_number, CONCAT('TKT-', t.id)) as ticket_number
                FROM crm_tickets t
                WHERE t.customer_no IN ('{customer_ids}')
                AND t.created_at >= CURRENT_DATE - INTERVAL '2 years'
                ORDER BY t.created_at DESC
                LIMIT 5000
            """
            
            cursor.execute(ticket_query)
            ticket_records = cursor.fetchall()
            
            stored_count = 0
            
            # Store tickets in batches
            for ticket_row in ticket_records:
                try:
                    ticket_data = dict(ticket_row)
                    customer_id = ticket_data['customer_id']
                    
                    # Get internal customer ID
                    internal_customer_id = self.customer_cache.get(str(customer_id))
                    if not internal_customer_id:
                        continue
                    
                    # Check if ticket already exists
                    existing_ticket = Ticket.query.filter_by(
                        company_id=self.company.id,
                        ticket_number=ticket_data.get('ticket_number', f"crm_{ticket_data['id']}")
                    ).first()
                    
                    if not existing_ticket:
                        # Create new ticket record
                        ticket = Ticket(
                            company_id=self.company.id,
                            customer_id=internal_customer_id,
                            title=ticket_data.get('title', 'Support Request')[:200],
                            description=ticket_data.get('description', '')[:1000],
                            status=ticket_data.get('status', 'open'),
                            priority=ticket_data.get('priority', 'medium'),
                            ticket_number=ticket_data.get('ticket_number', f"crm_{ticket_data['id']}"),
                            created_at=ticket_data['created_at'] or datetime.utcnow(),
                            updated_at=ticket_data.get('updated_at', datetime.utcnow())
                        )
                        
                        db.session.add(ticket)
                        stored_count += 1
                        
                        if stored_count % 50 == 0:
                            db.session.commit()
                    
                except Exception as e:
                    logger.warning(f"Ticket storage error: {e}")
                    self.sync_stats['tickets']['errors'] += 1
                    continue
            
            # Final commit
            db.session.commit()
            
            self.sync_stats['tickets']['stored'] = stored_count
            self.query_times['ticket_storage'] = round(time.time() - start_time, 2)
            
            logger.info(f"   âœ… Stored {stored_count:,} ticket records in {self.query_times['ticket_storage']}s")
            
        except Exception as e:
            logger.error(f"Ticket storage failed: {e}")
            self.sync_stats['tickets']['errors'] += 1
    
    def _store_usage_statistics(self, cursor):
        """Store usage statistics summary (not individual records due to volume)"""
        
        try:
            logger.info("   â†’ Storing usage statistics summary...")
            start_time = time.time()
            
            # Store usage summary rather than individual records (too many)
            # This could be enhanced to create a UsageStatistic model if needed
            
            stored_count = len(self.customer_cache)  # One summary per customer
            self.sync_stats['usage_stats']['stored'] = stored_count
            self.query_times['usage_storage'] = round(time.time() - start_time, 2)
            
            logger.info(f"   âœ… Usage statistics included in customer records ({stored_count:,} summaries)")
            
        except Exception as e:
            logger.error(f"Usage statistics storage failed: {e}")
            self.sync_stats['usage_stats']['errors'] += 1
    
    def _generate_disconnection_based_predictions(self):
        """Generate disconnection-based churn predictions for all customers"""
        
        try:
            logger.info(f"Generating disconnection-based predictions for {len(self.enhanced_customers)} customers...")
            
            predictions_generated = 0
            
            for crm_id, enhanced_data in self.enhanced_customers.items():
                try:
                    internal_customer_id = self.customer_cache.get(crm_id)
                    if not internal_customer_id:
                        continue
                    
                    # Generate prediction using disconnection-based data
                    prediction_result = {
                        'churn_risk': enhanced_data['predicted_churn_risk'],
                        'churn_probability': enhanced_data['churn_probability'],
                        'risk_factors': enhanced_data['risk_reasoning'],
                        'disconnection_based': True,
                        'days_since_disconnection': enhanced_data['days_since_disconnection'],
                        'disconnection_status': enhanced_data['disconnection_risk_level']
                    }
                    
                    # Create prediction record
                    prediction = Prediction.create_prediction(
                        company_id=self.company.id,
                        customer_id=crm_id,
                        prediction_result=prediction_result
                    )
                    
                    if prediction:
                        predictions_generated += 1
                        
                        # Update risk counters
                        risk_level = prediction_result['churn_risk']
                        self.sync_stats['predictions'][f'{risk_level}_risk'] += 1
                        
                        # Track high risk disconnected customers
                        if enhanced_data['disconnection_date'] and risk_level == 'high':
                            self.sync_stats['disconnection_analysis']['high_risk_disconnected'] += 1
                        elif enhanced_data['disconnection_date'] and risk_level == 'medium':
                            self.sync_stats['disconnection_analysis']['medium_risk_disconnected'] += 1
                        
                        # Update customer record
                        customer = Customer.query.filter_by(
                            company_id=self.company.id,
                            crm_customer_id=crm_id
                        ).first()
                        
                        if customer:
                            customer.churn_risk = risk_level
                            customer.churn_probability = prediction_result['churn_probability']
                            customer.last_prediction_date = datetime.utcnow()
                            customer.days_since_disconnection = enhanced_data['days_since_disconnection']
                    
                except Exception as e:
                    logger.warning(f"Prediction error for customer {crm_id}: {e}")
                    self.sync_stats['predictions']['errors'] += 1
                    continue
            
            self.sync_stats['predictions']['generated'] = predictions_generated
            
            try:
                db.session.commit()
                logger.info(f"âœ… Generated {predictions_generated} disconnection-based predictions")
                logger.info(f"   High risk: {self.sync_stats['predictions']['high_risk']}")
                logger.info(f"   Medium risk: {self.sync_stats['predictions']['medium_risk']}")
                logger.info(f"   Low risk: {self.sync_stats['predictions']['low_risk']}")
            except Exception as e:
                logger.error(f"Failed to commit predictions: {e}")
                db.session.rollback()
            
        except Exception as e:
            logger.error(f"Disconnection-based prediction generation failed: {e}")
            raise
    
    def _analyze_disconnection_patterns(self):
        """Analyze disconnection patterns for business insights"""
        
        try:
            disconnection_stats = self.sync_stats['disconnection_analysis']
            
            logger.info("ðŸ“Š DISCONNECTION ANALYSIS SUMMARY:")
            logger.info(f"   Total disconnected customers: {disconnection_stats['total_disconnected']:,}")
            logger.info(f"   Disconnected 90+ days (High Risk): {disconnection_stats['days_90_plus']:,}")
            logger.info(f"   Disconnected 60+ days: {disconnection_stats['days_60_plus']:,}")
            logger.info(f"   High risk disconnected: {disconnection_stats['high_risk_disconnected']:,}")
            logger.info(f"   Medium risk disconnected: {disconnection_stats['medium_risk_disconnected']:,}")
            
            if disconnection_stats['total_disconnected'] > 0:
                high_risk_pct = (disconnection_stats['high_risk_disconnected'] / disconnection_stats['total_disconnected']) * 100
                logger.info(f"   High risk percentage: {high_risk_pct:.1f}%")
            
        except Exception as e:
            logger.warning(f"Disconnection analysis failed: {e}")
    
    # Helper methods (same as optimized service)
    
    def _safe_date_calculation(self, created_at, current_date):
        try:
            if created_at is None:
                return 12.0, '2023-01-01'
            
            if isinstance(created_at, datetime):
                created_dt = created_at
            elif isinstance(created_at, str):
                date_formats = [
                    '%d/%m/%Y %H:%M:%S', '%d/%m/%Y', '%Y-%m-%d %H:%M:%S', 
                    '%Y-%m-%d', '%m/%d/%Y %H:%M:%S', '%m/%d/%Y'
                ]
                
                created_dt = None
                for date_format in date_formats:
                    try:
                        created_dt = datetime.strptime(created_at, date_format)
                        break
                    except ValueError:
                        continue
                
                if created_dt is None:
                    return 12.0, '2023-01-01'
            else:
                return 12.0, '2023-01-01'
            
            tenure_months = (current_date - created_dt).days / 30.44
            signup_date = created_dt.strftime('%Y-%m-%d')
            
            return max(tenure_months, 0.1), signup_date
            
        except Exception as e:
            return 12.0, '2023-01-01'
    
    def _update_customer_with_disconnection_data(self, customer, enhanced_data):
        """Update customer with disconnection data"""
        customer.customer_name = enhanced_data.get('customer_name', customer.customer_name)
        customer.phone = enhanced_data.get('phone', customer.phone)
        customer.outstanding_balance = enhanced_data.get('outstanding_balance', customer.outstanding_balance)
        customer.address = enhanced_data.get('address', customer.address)
        customer.total_payments = enhanced_data.get('total_payments', customer.total_payments or 0)
        customer.total_tickets = enhanced_data.get('total_tickets', customer.total_tickets or 0)
        customer.tenure_months = enhanced_data.get('tenure_months', customer.tenure_months)
        
        # NEW: Disconnection fields
        customer.disconnection_date = enhanced_data.get('disconnection_date')
        customer.days_since_disconnection = enhanced_data.get('days_since_disconnection', 0)
        customer.payment_consistency_score = enhanced_data.get('payment_consistency_score', 1.0)
        customer.last_payment_date = self._parse_date(enhanced_data.get('last_payment_date'))
        customer.days_since_last_payment = enhanced_data.get('days_since_last_payment', 0)
        
        customer.updated_at = datetime.utcnow()
        customer.synced_at = datetime.utcnow()
    
    def _create_customer_with_disconnection_data(self, enhanced_data):
        """Create customer with disconnection data"""
        customer = Customer(
            company_id=self.company.id,
            crm_customer_id=enhanced_data['crm_customer_id'],
            customer_name=enhanced_data['customer_name'],
            phone=enhanced_data.get('phone'),
            email=enhanced_data.get('email'),
            address=enhanced_data.get('address'),
            outstanding_balance=enhanced_data.get('outstanding_balance', 0),
            status=enhanced_data.get('status', 'active'),
            signup_date=self._parse_date(enhanced_data.get('signup_date')),
            tenure_months=enhanced_data.get('tenure_months', 0),
            total_payments=enhanced_data.get('total_payments', 0),
            total_tickets=enhanced_data.get('total_tickets', 0),
            
            # NEW: Disconnection fields
            disconnection_date=enhanced_data.get('disconnection_date'),
            days_since_disconnection=enhanced_data.get('days_since_disconnection', 0),
            payment_consistency_score=enhanced_data.get('payment_consistency_score', 1.0),
            last_payment_date=self._parse_date(enhanced_data.get('last_payment_date')),
            days_since_last_payment=enhanced_data.get('days_since_last_payment', 0),
            
            created_at=datetime.utcnow(),
            synced_at=datetime.utcnow()
        )
        db.session.add(customer)
        db.session.flush()
        return customer
    
    # All other helper methods remain the same
    def _build_comprehensive_customer_cache(self):
        cache_start = time.time()
        logger.info("Building comprehensive customer cache...")
        
        try:
            customers = Customer.query.filter_by(company_id=self.company.id).all()
            
            for customer in customers:
                if customer.crm_customer_id:
                    self.customer_cache[customer.crm_customer_id] = customer.id
                    self.customer_name_cache[customer.crm_customer_id] = customer.customer_name
                
                self.customer_cache[str(customer.id)] = customer.id
            
            cache_time = time.time() - cache_start
            self.sync_stats['cache_performance']['build_time'] = cache_time
            
            logger.info(f"Customer cache built: {len(self.customer_cache)} entries in {cache_time:.2f}s")
            
        except Exception as e:
            logger.error(f"Failed to build customer cache: {e}")
            raise
    
    def _get_postgresql_connection(self):
        if self.connection and not self.connection.closed:
            return self.connection
        
        try:
            pg_config = self.company.get_postgresql_config()
            
            self.connection = psycopg2.connect(
                host=pg_config['host'],
                port=int(pg_config['port']),
                dbname=pg_config['database'],
                user=pg_config['username'],
                password=pg_config['password']
            )
            self.connection.autocommit = True
            
            logger.info("PostgreSQL connection established for disconnection-based sync")
            return self.connection
            
        except Exception as e:
            logger.error(f"PostgreSQL connection failed: {e}")
            raise
    
    def _safe_session_commit(self):
        try:
            db.session.commit()
        except Exception as e:
            logger.warning(f"Session commit failed: {e}")
            db.session.rollback()
            raise
    
    def _safe_session_rollback(self):
        try:
            db.session.rollback()
        except Exception as e:
            logger.warning(f"Session rollback warning: {e}")
    
    @staticmethod
    def _parse_date(date_string):
        if not date_string:
            return None
        
        date_str = str(date_string).strip()
        
        if date_str in ['0000-00-00', '0000-00-00 00:00:00', 'None', '']:
            return None
        
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except (ValueError, AttributeError):
            try:
                return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
            except (ValueError, AttributeError):
                try:
                    return datetime.strptime(date_str, '%Y-%m-%d')
                except (ValueError, AttributeError):
                    return None
    
    def test_postgresql_connection(self):
        logger.info("Testing DISCONNECTION-BASED PostgreSQL connection")
        
        try:
            pg_config = self.company.get_postgresql_config()
            
            if not all([pg_config['host'], pg_config['database'], pg_config['username'], pg_config['password']]):
                return {
                    'success': False,
                    'message': 'PostgreSQL configuration incomplete'
                }
            
            pg_config_fixed = {
                'host': pg_config['host'],
                'port': pg_config['port'], 
                'dbname': pg_config['database'],
                'user': pg_config['username'],
                'password': pg_config['password']
            }
            
            with psycopg2.connect(**pg_config_fixed) as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
                    cursor.execute("SELECT version()")
                    version = cursor.fetchone()[0]
                    
                    # Check disconnection data availability
                    cursor.execute("""
                        SELECT 
                            COUNT(*) as total_customers,
                            COUNT(churned_date) as disconnected_customers,
                            COUNT(CASE WHEN churned_date IS NOT NULL AND 
                                  EXTRACT(DAY FROM CURRENT_DATE - churned_date) >= 90 THEN 1 END) as high_risk_candidates
                        FROM crm_customers
                        WHERE customer_name IS NOT NULL AND customer_name != ''
                    """)
                    
                    disconnect_stats = cursor.fetchone()
                    
                    table_info = {}
                    tables = ['crm_customers', 'crm_tickets', 'nav_mpesa_transactions', 'spl_statistics']
                    
                    for table in tables:
                        try:
                            cursor.execute(f"SELECT COUNT(*) as count FROM {table}")
                            count = cursor.fetchone()['count']
                            table_info[table] = {'count': count, 'available': True}
                        except Exception as e:
                            table_info[table] = {'count': 0, 'available': False, 'error': str(e)}
                    
                    return {
                        'success': True,
                        'message': 'ðŸ”¥ DISCONNECTION-BASED PostgreSQL connection successful!',
                        'database_version': version,
                        'tables': table_info,
                        'disconnection_analysis': {
                            'total_customers': disconnect_stats['total_customers'],
                            'disconnected_customers': disconnect_stats['disconnected_customers'],
                            'high_risk_candidates': disconnect_stats['high_risk_candidates'],
                            'disconnection_rate': f"{(disconnect_stats['disconnected_customers']/disconnect_stats['total_customers']*100):.1f}%" if disconnect_stats['total_customers'] > 0 else '0%'
                        },
                        'enhanced_features': [
                            'ðŸ”¥ DISCONNECTION-BASED: Uses actual churned_date from CRM',
                            'ðŸ“Š BUSINESS LOGIC: 90/60 day disconnection rules',
                            'ðŸ’¾ DATA STORAGE: Saves payments, tickets, usage to SQLite',
                            'âš¡ OPTIMIZED: Fast separate queries',
                            'Real-time disconnection analysis',
                            'Payment behavior post-disconnection tracking'
                        ]
                    }
                    
        except Exception as e:
            return {
                'success': False,
                'message': f'DISCONNECTION-BASED PostgreSQL connection failed: {str(e)}'
            }


# Maintain backward compatibility
UltraFixedCRMService = DisconnectionBasedCRMService
CRMService = DisconnectionBasedCRMService
EnhancedCRMService = DisconnectionBasedCRMService
EnhancedCRMServiceWithPredictions = DisconnectionBasedCRMService
